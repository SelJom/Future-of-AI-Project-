# Configuration for Local Inference
# If using Ollama, standard port is 11434
LLM_BASE_URL=http://localhost:11434/v1
LLM_API_KEY=ollama
LLM_MODEL=llama3
# LLM_MODEL=mistral # Alternative if llama3 is too heavy

# Vector Store
CHROMA_DB_PATH=./data/chroma_db