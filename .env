# Configuration for Local Inference
# .env file
LLM_BASE_URL=http://localhost:11434/v1
LLM_API_KEY=ollama
# CHANGE THIS LINE:
LLM_MODEL=llama3.2-vision
CHROMA_DB_PATH=./data/chroma_db