import json
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from app.llm import get_llm
from app.vector_store import query_trials

llm = get_llm()

# --- 1. SUPERVISOR (Le Chef d'Orchestre) ---
def supervisor_node(state):
    """
    Analyse l'intention. Si c'est m√©dical, on lance la cha√Æne complexe.
    Sinon, on r√©pond simplement.
    """
    messages = state.get("messages", [])
    query = messages[-1].content if messages else ""
    
    system_prompt = """
    Tu es le Superviseur d'une IA m√©dicale. Analyse la demande de l'utilisateur.
    
    SI la demande concerne :
    - Une maladie, un sympt√¥me, un m√©dicament.
    - Une explication d'ordonnance (OCR context).
    - Une question de sant√© complexe.
    -> R√©ponds JSON: {"next_step": "MEDICAL_CHAIN"}
    
    SINON (Salutations, blagues, questions hors sujet) :
    -> R√©ponds JSON: {"next_step": "GENERAL_CHAT"}
    """
    
    try:
        resp = llm.invoke([SystemMessage(content=system_prompt), HumanMessage(content=query)])
        decision = json.loads(resp.content.replace("```json", "").replace("```", "").strip())
        return {"next_step": decision.get("next_step", "GENERAL_CHAT"), "iteration_count": 0}
    except:
        return {"next_step": "GENERAL_CHAT", "iteration_count": 0}

# --- 2. MEDICAL EXPERT (L'Analyste Factuel) ---
def medical_expert_node(state):
    """
    R√©cup√®re la v√©rit√© scientifique (via RAG ou connaissances brutes).
    Ne simplifie PAS. Cherche l'exactitude.
    """
    query = state["messages"][-1].content
    # Simulation RAG (ou votre vraie fonction query_trials)
    retrieved = query_trials(query) 
    facts = f"Documents RAG: {retrieved}\n\nConnaissances LLM brutes sur: {query}"
    
    return {"medical_facts": facts}

# --- 3. PROFILER (L'Anthropologue) ---
def profiler_node(state):
    """
    Transforme les donn√©es d√©mographiques en strat√©gie de communication.
    """
    profile = state.get("user_profile", {})
    age = profile.get("age", 30)
    lang = profile.get("language", "Fran√ßais")
    level = profile.get("literacy_level", "Moyen")
    
    prompt = f"""
    Tu es un expert en communication interculturelle et sant√© publique (Health Literacy).
    
    Patient: {age} ans. Langue: {lang}. Niveau lecture: {level}.
    
    D√©finis une strat√©gie de r√©daction en 3 points :
    1. Ton (Empathique, Direct, Formel ?)
    2. M√©taphores culturelles adapt√©es (ex: m√©canique pour un ing√©nieur, nature pour contexte rural, etc. - INVENTE selon le profil).
    3. Tabous √† √©viter ou pr√©cautions de langage.
    
    R√©ponds uniquement avec la strat√©gie.
    """
    
    strategy = llm.invoke([HumanMessage(content=prompt)]).content
    return {"cultural_strategy": strategy}

# --- 4. TRANSLATOR (Le P√©dagogue) ---
def translator_node(state):
    """
    R√©dige l'explication en combinant Faits + Strat√©gie + (Optionnel) Critiques pr√©c√©dentes.
    """
    facts = state["medical_facts"]
    strategy = state["cultural_strategy"]
    feedback = state.get("critique_feedback", "Aucune critique pour l'instant.")
    messages = state["messages"]
    
    prompt = f"""
    Tu es le 'Health Literacy Translator'. 
    
    TA MISSION : R√©diger une r√©ponse pour le patient.
    
    SOURCES M√âDICALES (Ne rien inventer) :
    {facts}
    
    STRAT√âGIE DE COMMUNICATION :
    {strategy}
    
    FEEDBACK DU GUARDIAN (Corrections √† appliquer si n√©cessaire) :
    {feedback}
    
    R√©dige la r√©ponse maintenant (en {state['user_profile'].get('language')}).
    """
    
    # On garde l'historique des messages pour le contexte
    response = llm.invoke([SystemMessage(content=prompt)] + messages[-2:]) # Contexte court
    return {"draft_response": response.content}

# --- 5. GUARDIAN (Le Superviseur de S√©curit√© - Boucle de R√©troaction) ---
def guardian_node(state):
    """
    V√©rifie si la simplification n'a pas d√©form√© la v√©rit√© m√©dicale ou omis un danger.
    """
    facts = state["medical_facts"]
    draft = state["draft_response"]
    
    prompt = f"""
    Tu es le Docteur Superviseur (Safety Guardian).
    
    1. FAITS M√âDICAUX ORIGINAUX : {facts}
    2. BROUILLON SIMPLIFI√â PROPOS√â : {draft}
    
    T√ÇCHE : D√©tecte les erreurs graves.
    - Hallucination (Le brouillon dit un truc non pr√©sent dans les faits ?)
    - Omission dangereuse (Un effet secondaire grave a disparu ?)
    - Infantilisation excessive ou ton inappropri√© ?
    
    R√©ponds JSON :
    {{
        "status": "APPROVED" ou "REJECTED",
        "feedback": "Si REJECTED, explique pr√©cis√©ment quoi corriger. Si APPROVED, mets 'RAS'."
    }}
    """
    
    try:
        resp = llm.invoke([HumanMessage(content=prompt)])
        analysis = json.loads(resp.content.replace("```json", "").replace("```", "").strip())
        return {
            "safety_status": analysis.get("status", "REJECTED"),
            "critique_feedback": analysis.get("feedback", "Erreur parsing"),
            "iteration_count": state["iteration_count"] + 1
        }
    except:
        # En cas de doute, on rejette
        return {"safety_status": "REJECTED", "critique_feedback": "Format JSON invalide, r√©essaie.", "iteration_count": state["iteration_count"] + 1}

# --- 6. VISUALIZER (Le G√©n√©rateur d'Image Mentale) ---
def visualizer_node(state):
    """
    G√©n√®re un prompt pour une image qui aide √† comprendre.
    """
    draft = state["draft_response"]
    
    prompt = f"""
    Analyse cette explication m√©dicale : "{draft}"
    
    Cr√©e une description pour une image √©ducative (infographie ou illustration simple) qui aiderait √† comprendre le concept cl√©.
    Pas de texte dans l'image, juste du visuel.
    
    Exemple : "Un dessin sch√©matique de poumons agissant comme des √©ponges..."
    
    R√©ponds avec le prompt de l'image uniquement.
    """
    
    vis_prompt = llm.invoke([HumanMessage(content=prompt)]).content
    
    # Ici, nous appendons finalement la r√©ponse au fil de discussion
    final_content = f"{draft}\n\n---\n*üé® Id√©e visuelle sugg√©r√©e par l'IA : {vis_prompt}*"
    
    return {
        "visual_prompt": vis_prompt,
        "messages": [SystemMessage(content=final_content)] # C'est ici qu'on finalise
    }

# --- AGENT SIMPLE (Pour le "Bonjour") ---
def general_chat_node(state):
    return {"messages": [llm.invoke(state["messages"])]}